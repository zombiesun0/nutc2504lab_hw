import os
import json
import operator
from typing import TypedDict, List, Annotated

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

from search_searxng import search_searxng
from vlm_read_website import vlm_read_website

# åˆå§‹åŒ– LLM
llm = ChatOpenAI(
    base_url = "https://ws-05.huannago.com/v1",
    api_key = "vllm-token",
    model="Qwen3-VL-8B-Instruct-BF16.gguf",
    temperature= 0
)

CACHE_FILE = "qa_cache.json"

# ================= å¿«å–èˆ‡å·¥å…·å‡½å¼ =================

def get_clean_key(text: str) -> str:
    return text.replace(" ", "").replace("?", "")

def load_cache():
    if not os.path.exists(CACHE_FILE): return {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {}

def save_cache(new_data: dict):
    current = load_cache()
    current.update(new_data)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(current, f, ensure_ascii=False, indent=4)

# ================= å®šç¾©ç‹€æ…‹ (State) =================

class AgentState(TypedDict):
    question: str
    answer: str
    knowledge_base: Annotated[List[str], operator.add] # ç´¯ç©æœ‰åƒ¹å€¼çš„è³‡è¨Š
    current_query: str 
    loop_count: int
    source: str # CACHE / SEARCH / PLANNER

# ================= å®šç¾©ç¯€é» (Nodes) =================

def check_cache_node(state: AgentState):
    """å¿«å–æª¢æŸ¥"""
    print(f"\n[1] æª¢æŸ¥å¿«å–: {state['question']}")
    cache = load_cache()
    key = get_clean_key(state['question'])
    
    if key in cache:
        print("   -> å‘½ä¸­å¿«å– (Hit)")
        return {"answer": cache[key], "source": "CACHE"}
    else:
        print("   -> å¿«å–æœªå‘½ä¸­ (Miss)")
        # åˆå§‹åŒ– knowledge_base ç‚ºç©ºåˆ—è¡¨
        return {"source": "SEARCH", "knowledge_base": [], "loop_count": 0}

def planner_node(state: AgentState):
    """è¦åŠƒå™¨ï¼šæ±ºå®šç¹¼çºŒæœå°‹æˆ–å›ç­”"""
    print(f"[2] Planner è©•ä¼°ä¸­ (Loop: {state.get('loop_count', 0)})...")
   
    if state.get('loop_count', 0) >= 3: # å®‰å…¨æ©Ÿåˆ¶ï¼šè¶…é 3 æ¬¡å¼·åˆ¶å›ç­”
        print("   -> é”æœ€å¤§è¿´åœˆæ•¸ï¼Œå¼·åˆ¶å›ç­”ã€‚")
        return {"source": "force_answer"}

    if not state.get('knowledge_base'):  # å¦‚æœå®Œå…¨æ²’çŸ¥è­˜
        print("   -> å°šç„¡è³‡è¨Šï¼Œéœ€è¦æœå°‹ã€‚")
        return {"source": "need_search"}
   
    context_str = "\n".join(state['knowledge_base']) # è®“ LLM åˆ¤æ–·è³‡è¨Šè¶³å¤ èˆ‡å¦
    prompt = f"""
    å•é¡Œ: {state['question']}
    ç›®å‰å·²çŸ¥è³‡è¨Š:
    {context_str}
    
    è«‹å•ä¸Šè¿°è³‡è¨Šæ˜¯å¦å·²ç¶“è¶³å¤ å®Œæ•´å›ç­”å•é¡Œï¼Ÿ
    å›ç­” "YES" è¡¨ç¤ºè¶³å¤ ï¼Œå›ç­” "NO" è¡¨ç¤ºéœ€è¦æ›´å¤šè³‡è¨Šã€‚
    """
    judge = llm.invoke([HumanMessage(content=prompt)]).content.strip().upper()
    
    if "YES" in judge:
        print("   -> è³‡è¨Šå……è¶³ï¼Œæº–å‚™å›ç­”ã€‚")
        return {"source": "ready_to_answer"}
    else:
        print("   -> è³‡è¨Šä¸è¶³ï¼Œç¹¼çºŒæœå°‹ã€‚")
        return {"source": "need_search"}

def query_gen_node(state: AgentState):
    """ç”Ÿæˆé—œéµå­—"""
    print("[3] ç”Ÿæˆæœå°‹é—œéµå­—...")
    prompt = f"åŸºæ–¼å•é¡Œ '{state['question']}' èˆ‡å·²çŸ¥è³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹æœ€é‡è¦çš„æœå°‹é—œéµå­—ã€‚"
    query = llm.invoke([HumanMessage(content=prompt)]).content.strip()
    print(f"   -> é—œéµå­—: {query}")
    
    return {"current_query": query, "loop_count": state.get("loop_count", 0) + 1}

def search_node(state: AgentState):
    """æœå°‹ + VLM è®€å–"""
    query = state.get("current_query", state["question"])
    print(f"[4] åŸ·è¡Œæœå°‹: {query}")
    
    # 1. åŸ·è¡Œæœå°‹
    results = search_searxng(query=query, limit=2) 
    
    print("    -> VLM è®€å–ç¶²é ä¸¦è©•ä¼°åƒ¹å€¼...")
    new_knowledge = []
    
    for res in results:
        url = res.get("url")
        title = res.get("title", "ç¶²é ")
        
        try:          
            content = vlm_read_website(url, title) #å‘¼å« VLM è®€å–
            
            # è©•ä¼°åƒ¹å€¼
            check_prompt = f"""
            å•é¡Œ: {state['question']}
            ç¶²é å…§å®¹: {content[:1000]}... (ç•¥)
            
            é€™æ®µå…§å®¹å°å›ç­”å•é¡Œæœ‰å¹«åŠ©å—ï¼Ÿæœ‰åƒ¹å€¼è«‹å›ç­” YESï¼Œå¦å‰‡å›ç­” NOã€‚
            """
            valuable = llm.invoke([HumanMessage(content=check_prompt)]).content.strip().upper()
                        
            if "YES" in valuable: # 4. è‹¥æœ‰åƒ¹å€¼ -> åŠ å…¥åˆ—è¡¨
                print(f"       [V] ç™¼ç¾æœ‰åƒ¹å€¼è³‡è¨Š: {title}")
                summary = f"ä¾†æº {title}: {content[:300]}..." 
                new_knowledge.append(summary)
            else:
                print(f"       [X] è³‡è¨Šé—œè¯åº¦ä½: {title}")
        except Exception as e:
            print(f"       [!] è®€å–å¤±æ•— {url}: {e}")
            
    # LangGraph æœƒè‡ªå‹• operator.add å°‡é€™è£¡å›å‚³çš„ list èˆ‡åŸæœ¬çš„ knowledge_base ç›¸åŠ 
    return {"knowledge_base": new_knowledge}

def final_node(state: AgentState):
    """ç”Ÿæˆæœ€çµ‚å›ç­”ä¸¦å¯«å…¥å¿«å–"""
    print("[5] ç”Ÿæˆæœ€çµ‚å›ç­”...")
   
    if state.get("source") == "CACHE":  # å¦‚æœæ˜¯å¾ Cache ä¾†çš„ 
        return {}

    context = "\n".join(state.get('knowledge_base', []))
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹æ”¶é›†åˆ°çš„è³‡è¨Šå›ç­”å•é¡Œï¼š
    å•é¡Œ: {state['question']}
    è³‡è¨Š: {context}
    """
    final_ans = llm.invoke([HumanMessage(content=prompt)]).content
    
    # å¯«å…¥å¿«å–
    save_cache({get_clean_key(state['question']): final_ans})
    print("   -> å·²æ›´æ–°å¿«å–ã€‚")
    
    return {"answer": final_ans}

# ================= æ§‹å»º =================

workflow = StateGraph(AgentState)

# æ–°å¢ç¯€é»
workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_node)
workflow.add_node("final_answer", final_node) 

workflow.set_entry_point("check_cache")

#Cache -> Planner æˆ– End
def route_cache(state):
    if state["source"] == "CACHE": return "end_flow" 
    return "planner"

workflow.add_conditional_edges(
    "check_cache",
    route_cache,
    {
        "end_flow": END,
        "planner": "planner"
    }
)

# Planner -> Final Answer æˆ– Query Gen
def route_planner(state):

    if state["source"] == "ready_to_answer" or state["source"] == "force_answer":
        return "final_answer" 
    return "query_gen"

workflow.add_conditional_edges(
    "planner", 
    route_planner,
    {
        "final_answer": "final_answer",
        "query_gen": "query_gen"
    }
)

# å»ºç«‹å¾ªç’°
workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")

# çµæŸ
workflow.add_edge("final_answer", END) # [ä¿®æ­£] ä½¿ç”¨ final_answer

# ç·¨è­¯
app = workflow.compile()

# ================= åŸ·è¡Œ =================
if __name__ == "__main__":
    print(app.get_graph().draw_ascii()) 
    
    while True:
        user_q = input("\nè«‹è¼¸å…¥å•é¡Œ (q é›¢é–‹): ")
        if user_q.lower() == "q": break

        inputs = {"question": user_q}
        try:
            result = app.invoke(inputs)
            print(f"\nğŸ’¡ æœ€çµ‚å›ç­”: {result['answer']}")
        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")